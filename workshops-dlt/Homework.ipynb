{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "082f595c-7744-48e2-8fe2-30db84fcac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install dlt[lancedb]==0.5.1a0\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b94c2e-5f4d-42ae-ad7b-634f03d5ef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents\n",
      "[{'name': 'text', 'data_type': 'text', 'nullable': True}, {'name': 'section', 'data_type': 'text', 'nullable': True}, {'name': 'question', 'data_type': 'text', 'nullable': True}, {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True}]\n",
      "_dlt_version\n",
      "[{'name': 'version', 'data_type': 'bigint', 'nullable': False}, {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'schema_name', 'data_type': 'text', 'nullable': False}, {'name': 'version_hash', 'data_type': 'text', 'nullable': False}, {'name': 'schema', 'data_type': 'text', 'nullable': False}]\n",
      "_dlt_loads\n",
      "[{'name': 'load_id', 'data_type': 'text', 'nullable': False}, {'name': 'schema_name', 'data_type': 'text', 'nullable': True}, {'name': 'status', 'data_type': 'bigint', 'nullable': False}, {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'schema_version_hash', 'data_type': 'text', 'nullable': True}]\n",
      "_dlt_pipeline_state\n",
      "[{'name': 'version', 'data_type': 'bigint', 'nullable': False}, {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, {'name': 'pipeline_name', 'data_type': 'text', 'nullable': False}, {'name': 'state', 'data_type': 'text', 'nullable': False}, {'name': 'created_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'version_hash', 'data_type': 'text', 'nullable': True}, {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True}]\n",
      "UPLOAD\n",
      "Pipeline from_json load step completed in 0.57 seconds\n",
      "1 load package(s) were loaded to destination LanceDB and into dataset qanda\n",
      "The LanceDB destination used <dlt.destinations.impl.lancedb.configuration.LanceDBCredentials object at 0x757fcc2d33d0> location to store data\n",
      "Load package 1721154852.23457 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import dlt\n",
    "\n",
    "qa_dataset = requests.get(\"https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1\").json()\n",
    "\n",
    "@dlt.resource\n",
    "def qa_documents():\n",
    "  for course in qa_dataset:\n",
    "    yield course[\"documents\"]\n",
    "\n",
    "pipeline = dlt.pipeline(pipeline_name=\"from_json\", destination=\"lancedb\", dataset_name=\"qanda\")\n",
    "\n",
    "load_info = pipeline.run(qa_documents, table_name=\"documents\")\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f8474c-f177-4809-90ad-9aa01d38bfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qanda____dlt_loads', 'qanda____dlt_pipeline_state', 'qanda____dlt_version', 'qanda___dltSentinelTable', 'qanda___documents']\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"./.lancedb\")\n",
    "print(db.table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37c9cec4-ddc1-4108-a3a2-512b23b832c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id__</th>\n",
       "      <th>text</th>\n",
       "      <th>section</th>\n",
       "      <th>question</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2e281b43-8ab0-52fa-a4d0-1eaf1b410811</td>\n",
       "      <td>The purpose of this document is to capture fre...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - When will the course start?</td>\n",
       "      <td>1721154852.23457</td>\n",
       "      <td>g9aRDhvUBvbHNg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3771d95c-a079-535c-83b4-3335de542eae</td>\n",
       "      <td>GitHub - DataTalksClub data-engineering-zoomca...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What are the prerequisites for this c...</td>\n",
       "      <td>1721154852.23457</td>\n",
       "      <td>DNrlTHdW0TW8ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55416d01-d00e-5b55-9f19-d90e625e46f1</td>\n",
       "      <td>Yes, even if you don't register, you're still ...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - Can I still join the course after the...</td>\n",
       "      <td>1721154852.23457</td>\n",
       "      <td>vHQcUeBWbAxjeA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9d961972-140f-5e56-8374-a9f436227b94</td>\n",
       "      <td>You don't need it. You're accepted. You can al...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - I have registered for the Data Engine...</td>\n",
       "      <td>1721154852.23457</td>\n",
       "      <td>MJzsfFmRyiXNTw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fd64d4b5-f61b-5916-830f-81c69cde8c23</td>\n",
       "      <td>You can start by installing and setting up all...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What can I do before the course starts?</td>\n",
       "      <td>1721154852.23457</td>\n",
       "      <td>hOJODRZ6Y/14Kw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>484835d9-f606-5a73-a66c-f8a06dcc52ea</td>\n",
       "      <td>Problem description\\nThis is the step in the c...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Github actions: Permission denied error when e...</td>\n",
       "      <td>1721154852.23457</td>\n",
       "      <td>y4QYUQ89ROZS1A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>34be2362-311d-534d-afb4-8072e66a457f</td>\n",
       "      <td>Problem description\\nWhen a docker-compose fil...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Managing Multiple Docker Containers with docke...</td>\n",
       "      <td>1721154852.23457</td>\n",
       "      <td>jP7wFSLo9nfDQQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>aa0cbcc0-1189-5621-b904-e1c5a668cbeb</td>\n",
       "      <td>Problem description\\nIf you are having problem...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>AWS regions need to match docker-compose</td>\n",
       "      <td>1721154852.23457</td>\n",
       "      <td>A03we6dYLa8pHQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>9159b20a-02a4-50e9-8359-db19a303b021</td>\n",
       "      <td>Problem description\\nPre-commit command was fa...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Isort Pre-commit</td>\n",
       "      <td>1721154852.23457</td>\n",
       "      <td>LZ/0ViU33q681A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>8cd9abc1-c2df-5be3-85e7-25a0d0871cc7</td>\n",
       "      <td>Problem description\\nInfrastructure created in...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>How to destroy infrastructure created via GitH...</td>\n",
       "      <td>1721154852.23457</td>\n",
       "      <td>2i70ZH2HMCXY4Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>948 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id__  \\\n",
       "0    2e281b43-8ab0-52fa-a4d0-1eaf1b410811   \n",
       "1    3771d95c-a079-535c-83b4-3335de542eae   \n",
       "2    55416d01-d00e-5b55-9f19-d90e625e46f1   \n",
       "3    9d961972-140f-5e56-8374-a9f436227b94   \n",
       "4    fd64d4b5-f61b-5916-830f-81c69cde8c23   \n",
       "..                                    ...   \n",
       "943  484835d9-f606-5a73-a66c-f8a06dcc52ea   \n",
       "944  34be2362-311d-534d-afb4-8072e66a457f   \n",
       "945  aa0cbcc0-1189-5621-b904-e1c5a668cbeb   \n",
       "946  9159b20a-02a4-50e9-8359-db19a303b021   \n",
       "947  8cd9abc1-c2df-5be3-85e7-25a0d0871cc7   \n",
       "\n",
       "                                                  text  \\\n",
       "0    The purpose of this document is to capture fre...   \n",
       "1    GitHub - DataTalksClub data-engineering-zoomca...   \n",
       "2    Yes, even if you don't register, you're still ...   \n",
       "3    You don't need it. You're accepted. You can al...   \n",
       "4    You can start by installing and setting up all...   \n",
       "..                                                 ...   \n",
       "943  Problem description\\nThis is the step in the c...   \n",
       "944  Problem description\\nWhen a docker-compose fil...   \n",
       "945  Problem description\\nIf you are having problem...   \n",
       "946  Problem description\\nPre-commit command was fa...   \n",
       "947  Problem description\\nInfrastructure created in...   \n",
       "\n",
       "                              section  \\\n",
       "0    General course-related questions   \n",
       "1    General course-related questions   \n",
       "2    General course-related questions   \n",
       "3    General course-related questions   \n",
       "4    General course-related questions   \n",
       "..                                ...   \n",
       "943          Module 6: Best practices   \n",
       "944          Module 6: Best practices   \n",
       "945          Module 6: Best practices   \n",
       "946          Module 6: Best practices   \n",
       "947          Module 6: Best practices   \n",
       "\n",
       "                                              question      _dlt_load_id  \\\n",
       "0                 Course - When will the course start?  1721154852.23457   \n",
       "1    Course - What are the prerequisites for this c...  1721154852.23457   \n",
       "2    Course - Can I still join the course after the...  1721154852.23457   \n",
       "3    Course - I have registered for the Data Engine...  1721154852.23457   \n",
       "4     Course - What can I do before the course starts?  1721154852.23457   \n",
       "..                                                 ...               ...   \n",
       "943  Github actions: Permission denied error when e...  1721154852.23457   \n",
       "944  Managing Multiple Docker Containers with docke...  1721154852.23457   \n",
       "945           AWS regions need to match docker-compose  1721154852.23457   \n",
       "946                                   Isort Pre-commit  1721154852.23457   \n",
       "947  How to destroy infrastructure created via GitH...  1721154852.23457   \n",
       "\n",
       "            _dlt_id  \n",
       "0    g9aRDhvUBvbHNg  \n",
       "1    DNrlTHdW0TW8ow  \n",
       "2    vHQcUeBWbAxjeA  \n",
       "3    MJzsfFmRyiXNTw  \n",
       "4    hOJODRZ6Y/14Kw  \n",
       "..              ...  \n",
       "943  y4QYUQ89ROZS1A  \n",
       "944  jP7wFSLo9nfDQQ  \n",
       "945  A03we6dYLa8pHQ  \n",
       "946  LZ/0ViU33q681A  \n",
       "947  2i70ZH2HMCXY4Q  \n",
       "\n",
       "[948 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_table = db.open_table(\"qanda___documents\")\n",
    "\n",
    "db_table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a473dea1-55e5-4b7d-8163-dc4f48f3d3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up the init scripts in \u001b[1mhttps://github.com/dlt-hub/verified-sources.git\u001b[0m...\n",
      "Cloning and configuring a verified source \u001b[1mrest_api\u001b[0m (Generic API Source)\n",
      "Do you want to proceed? [Y/n]: \n",
      "Verified source \u001b[1mrest_api\u001b[0m was added to your project!\n",
      "* See the usage examples and code snippets to copy from \u001b[1mrest_api_pipeline.py\u001b[0m\n",
      "* Add credentials for \u001b[1mlancedb\u001b[0m and other secrets in \u001b[1m./.dlt/secrets.toml\u001b[0m\n",
      "* \u001b[1mrequirements.txt\u001b[0m was created. Install it with:\n",
      "pip3 install -r requirements.txt\n",
      "* Read \u001b[1mhttps://dlthub.com/docs/walkthroughs/create-a-pipeline\u001b[0m for more information\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!yes | dlt init rest_api lancedb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41e0800d-007a-416b-be49-f6089e9f8285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SOURCES__REST_API__NOTION__API_KEY\"] = \"notion api key\"\n",
    "\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL_PROVIDER\"] = \"sentence-transformers\"\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL\"] = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__URI\"] = \".lancedb\"\n",
    "os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__API_KEY\"] = \"api_key\"\n",
    "os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__EMBEDDING_MODEL_PROVIDER_API_KEY\"] = \"embedding_model_provider_api_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9be2ed0-c066-4d6e-a1ee-cd16206388d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key_file = '/workspaces/api_key.txt'  \n",
    "\n",
    "def read_api_key(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        api_key = f.read().strip()\n",
    "    return api_key\n",
    "\n",
    "# Read API key from file\n",
    "api_key = read_api_key(api_key_file)\n",
    "\n",
    "# Set environment variable for the Notion API key\n",
    "os.environ[\"SOURCES__REST_API__NOTION__API_KEY\"] = api_key\n",
    "\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL_PROVIDER\"] = \"sentence-transformers\"\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL\"] = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__URI\"] = \".lancedb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e342c402-9c37-470f-b5f1-762a3e9e0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from rest_api import RESTAPIConfig, rest_api_source\n",
    "\n",
    "from dlt.sources.helpers.rest_client.paginators import BasePaginator, JSONResponsePaginator\n",
    "from dlt.sources.helpers.requests import Response, Request\n",
    "\n",
    "from dlt.destinations.adapters import lancedb_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7885e880-e2fe-499b-92c4-b7edf6fb25f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline company_policies load step completed in ---\n",
      "0 load package(s) were loaded to destination LanceDB and into dataset None\n",
      "The LanceDB destination used <dlt.destinations.impl.lancedb.configuration.LanceDBCredentials object at 0x757f3e039330> location to store data\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "class PostBodyPaginator(BasePaginator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cursor = None\n",
    "\n",
    "    def update_state(self, response: Response) -> None:\n",
    "        # Assuming the API returns an empty list when no more data is available\n",
    "        if not response.json():\n",
    "            self._has_next_page = False\n",
    "        else:\n",
    "            self.cursor = response.json().get(\"next_cursor\")\n",
    "            if self.cursor is None:\n",
    "                self._has_next_page = False\n",
    "\n",
    "    def update_request(self, request: Request) -> None:\n",
    "        if request.json is None:\n",
    "            request.json = {}\n",
    "\n",
    "        # Add the cursor to the request body\n",
    "        request.json[\"start_cursor\"] = self.cursor\n",
    "\n",
    "@dlt.resource(name=\"employee_handbook\")\n",
    "def rest_api_notion_resource():\n",
    "    notion_config: RESTAPIConfig = {\n",
    "        \"client\": {\n",
    "            \"base_url\": \"https://api.notion.com/v1/\",\n",
    "            \"auth\": {\n",
    "                \"token\": dlt.secrets[\"sources.rest_api.notion.api_key\"]\n",
    "            },\n",
    "            \"headers\":{\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Notion-Version\": \"2022-06-28\"\n",
    "            }\n",
    "        },\n",
    "        \"resources\": [\n",
    "            {\n",
    "                \"name\": \"search\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"search\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"paginator\": PostBodyPaginator(),\n",
    "                    \"json\": {\n",
    "                        \"query\": \"workshop\",\n",
    "                        \"sort\": {\n",
    "                            \"direction\": \"ascending\",\n",
    "                            \"timestamp\": \"last_edited_time\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"data_selector\": \"results\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page_content\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"blocks/{page_id}/children\",\n",
    "                    \"paginator\": JSONResponsePaginator(),\n",
    "                    \"params\": {\n",
    "                        \"page_id\": {\n",
    "                            \"type\": \"resolve\",\n",
    "                            \"resource\": \"search\",\n",
    "                            \"field\": \"id\"\n",
    "                        }\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    yield from rest_api_source(notion_config,name=\"employee_handbook\")\n",
    "\n",
    "def extract_page_content(response):\n",
    "    block_id = response[\"id\"]\n",
    "    last_edited_time = response[\"last_edited_time\"]\n",
    "    block_type = response.get(\"type\", \"Not paragraph\")\n",
    "    if block_type != \"paragraph\":\n",
    "        content = \"\"\n",
    "    else:\n",
    "        try:\n",
    "            content = response[\"paragraph\"][\"rich_text\"][0][\"plain_text\"]\n",
    "        except IndexError:\n",
    "            content = \"\"\n",
    "    return {\n",
    "        \"block_id\": block_id,\n",
    "        \"block_type\": block_type,\n",
    "        \"content\": content,\n",
    "        \"last_edited_time\": last_edited_time,\n",
    "        \"inserted_at_time\": datetime.now(timezone.utc)\n",
    "    }\n",
    "\n",
    "@dlt.resource(\n",
    "    name=\"employee_handbook\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"block_id\",\n",
    "    columns={\"last_edited_time\":{\"dedup_sort\":\"desc\"}}\n",
    "    )\n",
    "def rest_api_notion_incremental(\n",
    "    last_edited_time = dlt.sources.incremental(\"last_edited_time\", initial_value=\"2024-06-26T08:16:00.000Z\",primary_key=(\"block_id\"))\n",
    "):\n",
    "    # last_value = last_edited_time.last_value\n",
    "    # print(last_value)\n",
    "\n",
    "    for block in rest_api_notion_resource.add_map(extract_page_content):\n",
    "        if not(len(block[\"content\"])):\n",
    "            continue\n",
    "        yield block\n",
    "\n",
    "def load_notion() -> None:\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"company_policies\",\n",
    "        destination=\"lancedb\",\n",
    "        dataset_name=\"notion_pages\",\n",
    "        # full_refresh=True\n",
    "    )\n",
    "\n",
    "    load_info = pipeline.run(\n",
    "        lancedb_adapter(\n",
    "            rest_api_notion_incremental,\n",
    "            embed=\"content\"\n",
    "        ),\n",
    "        table_name=\"employee_handbook\",\n",
    "        write_disposition=\"merge\"\n",
    "    )\n",
    "    print(load_info)\n",
    "\n",
    "load_notion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eed450ef-fbda-4aca-ac1a-5b31950fe685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline company_policies load step completed in ---\n",
      "0 load package(s) were loaded to destination LanceDB and into dataset None\n",
      "The LanceDB destination used <dlt.destinations.impl.lancedb.configuration.LanceDBCredentials object at 0x757f3e03ac50> location to store data\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "class PostBodyPaginator(BasePaginator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cursor = None\n",
    "\n",
    "    def update_state(self, response: Response) -> None:\n",
    "        # Assuming the API returns an empty list when no more data is available\n",
    "        if not response.json():\n",
    "            self._has_next_page = False\n",
    "        else:\n",
    "            self.cursor = response.json().get(\"next_cursor\")\n",
    "            if self.cursor is None:\n",
    "                self._has_next_page = False\n",
    "\n",
    "    def update_request(self, request: Request) -> None:\n",
    "        if request.json is None:\n",
    "            request.json = {}\n",
    "\n",
    "        # Add the cursor to the request body\n",
    "        request.json[\"start_cursor\"] = self.cursor\n",
    "\n",
    "@dlt.resource(name=\"employee_handbook\")\n",
    "def rest_api_notion_resource():\n",
    "    notion_config: RESTAPIConfig = {\n",
    "        \"client\": {\n",
    "            \"base_url\": \"https://api.notion.com/v1/\",\n",
    "            \"auth\": {\n",
    "                \"token\": dlt.secrets[\"sources.rest_api.notion.api_key\"]\n",
    "            },\n",
    "            \"headers\":{\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Notion-Version\": \"2022-06-28\"\n",
    "            }\n",
    "        },\n",
    "        \"resources\": [\n",
    "            {\n",
    "                \"name\": \"search\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"search\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"paginator\": PostBodyPaginator(),\n",
    "                    \"json\": {\n",
    "                        \"query\": \"workshop\",\n",
    "                        \"sort\": {\n",
    "                            \"direction\": \"ascending\",\n",
    "                            \"timestamp\": \"last_edited_time\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"data_selector\": \"results\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page_content\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"blocks/{page_id}/children\",\n",
    "                    \"paginator\": JSONResponsePaginator(),\n",
    "                    \"params\": {\n",
    "                        \"page_id\": {\n",
    "                            \"type\": \"resolve\",\n",
    "                            \"resource\": \"search\",\n",
    "                            \"field\": \"id\"\n",
    "                        }\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    yield from rest_api_source(notion_config,name=\"employee_handbook\")\n",
    "\n",
    "def extract_page_content(response):\n",
    "    block_id = response[\"id\"]\n",
    "    last_edited_time = response[\"last_edited_time\"]\n",
    "    block_type = response.get(\"type\", \"Not paragraph\")\n",
    "    if block_type != \"paragraph\":\n",
    "        content = \"\"\n",
    "    else:\n",
    "        try:\n",
    "            content = response[\"paragraph\"][\"rich_text\"][0][\"plain_text\"]\n",
    "        except IndexError:\n",
    "            content = \"\"\n",
    "    return {\n",
    "        \"block_id\": block_id,\n",
    "        \"block_type\": block_type,\n",
    "        \"content\": content,\n",
    "        \"last_edited_time\": last_edited_time,\n",
    "        \"inserted_at_time\": datetime.now(timezone.utc)\n",
    "    }\n",
    "\n",
    "@dlt.resource(\n",
    "    name=\"employee_handbook\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"block_id\",\n",
    "    columns={\"last_edited_time\":{\"dedup_sort\":\"desc\"}}\n",
    "    )\n",
    "def rest_api_notion_incremental(\n",
    "    last_edited_time = dlt.sources.incremental(\"last_edited_time\", initial_value=\"2024-06-26T08:16:00.000Z\",primary_key=(\"block_id\"))\n",
    "):\n",
    "    # last_value = last_edited_time.last_value\n",
    "    # print(last_value)\n",
    "\n",
    "    for block in rest_api_notion_resource.add_map(extract_page_content):\n",
    "        if not(len(block[\"content\"])):\n",
    "            continue\n",
    "        yield block\n",
    "\n",
    "def load_notion() -> None:\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"company_policies\",\n",
    "        destination=\"lancedb\",\n",
    "        dataset_name=\"notion_pages\",\n",
    "        # full_refresh=True\n",
    "    )\n",
    "\n",
    "    load_info = pipeline.run(\n",
    "        lancedb_adapter(\n",
    "            rest_api_notion_incremental,\n",
    "            embed=\"content\"\n",
    "        ),\n",
    "        table_name=\"employee_handbook\",\n",
    "        write_disposition=\"merge\"\n",
    "    )\n",
    "    print(load_info)\n",
    "\n",
    "load_notion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8c3b172-cf2d-48dc-bb90-e6466deaf3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline company_policies load step completed in ---\n",
      "0 load package(s) were loaded to destination LanceDB and into dataset None\n",
      "The LanceDB destination used <dlt.destinations.impl.lancedb.configuration.LanceDBCredentials object at 0x757f404fb610> location to store data\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "import dlt\n",
    "import requests\n",
    "\n",
    "class PostBodyPaginator(BasePaginator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cursor = None\n",
    "\n",
    "    def update_state(self, response: Response) -> None:\n",
    "        if not response.json():\n",
    "            self._has_next_page = False\n",
    "        else:\n",
    "            self.cursor = response.json().get(\"next_cursor\")\n",
    "            if self.cursor is None:\n",
    "                self._has_next_page = False\n",
    "\n",
    "    def update_request(self, request: Request) -> None:\n",
    "        if request.json is None:\n",
    "            request.json = {}\n",
    "\n",
    "        request.json[\"start_cursor\"] = self.cursor\n",
    "\n",
    "@dlt.resource(name=\"homework\")\n",
    "def rest_api_notion_resource_homework():\n",
    "    notion_config: RESTAPIConfig = {\n",
    "        \"client\": {\n",
    "            \"base_url\": \"https://api.notion.com/v1/\",\n",
    "            \"auth\": {\n",
    "                \"token\": dlt.secrets[\"sources.rest_api.notion.api_key\"]\n",
    "            },\n",
    "            \"headers\":{\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Notion-Version\": \"2022-06-28\"\n",
    "            }\n",
    "        },\n",
    "        \"resources\": [\n",
    "            {\n",
    "                \"name\": \"search\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"search\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"paginator\": PostBodyPaginator(),\n",
    "                    \"json\": {\n",
    "                        \"query\": \"Homework: Employee handbook\",\n",
    "                        \"sort\": {\n",
    "                            \"direction\": \"ascending\",\n",
    "                            \"timestamp\": \"last_edited_time\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"data_selector\": \"results\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page_content\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"blocks/{page_id}/children\",\n",
    "                    \"paginator\": JSONResponsePaginator(),\n",
    "                    \"params\": {\n",
    "                        \"page_id\": {\n",
    "                            \"type\": \"resolve\",\n",
    "                            \"resource\": \"search\",\n",
    "                            \"field\": \"id\"\n",
    "                        }\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    yield from rest_api_source(notion_config, name=\"homework\")\n",
    "\n",
    "def extract_page_content(response):\n",
    "    block_id = response[\"id\"]\n",
    "    last_edited_time = response[\"last_edited_time\"]\n",
    "    block_type = response.get(\"type\", \"Not paragraph\")\n",
    "    if block_type != \"paragraph\":\n",
    "        content = \"\"\n",
    "    else:\n",
    "        try:\n",
    "            content = response[\"paragraph\"][\"rich_text\"][0][\"plain_text\"]\n",
    "        except IndexError:\n",
    "            content = \"\"\n",
    "    return {\n",
    "        \"block_id\": block_id,\n",
    "        \"block_type\": block_type,\n",
    "        \"content\": content,\n",
    "        \"last_edited_time\": last_edited_time,\n",
    "        \"inserted_at_time\": datetime.now(timezone.utc)\n",
    "    }\n",
    "\n",
    "@dlt.resource(\n",
    "    name=\"homework\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"block_id\",\n",
    "    columns={\"last_edited_time\": {\"dedup_sort\": \"desc\"}}\n",
    ")\n",
    "def rest_api_notion_incremental_homework(\n",
    "    last_edited_time=dlt.sources.incremental(\"last_edited_time\", initial_value=\"2024-06-26T08:16:00.000Z\", primary_key=(\"block_id\"))\n",
    "):\n",
    "    for block in rest_api_notion_resource_homework.add_map(extract_page_content):\n",
    "        if not len(block[\"content\"]):\n",
    "            continue\n",
    "        yield block\n",
    "\n",
    "def load_notion_homework() -> None:\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"company_policies\",\n",
    "        destination=\"lancedb\",\n",
    "        dataset_name=\"notion_pages\",\n",
    "    )\n",
    "\n",
    "    load_info = pipeline.run(\n",
    "        lancedb_adapter(\n",
    "            rest_api_notion_incremental_homework,\n",
    "            embed=\"content\"\n",
    "        ),\n",
    "        table_name=\"homework\",\n",
    "        write_disposition=\"merge\"\n",
    "    )\n",
    "    print(load_info)\n",
    "\n",
    "load_notion_homework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61646b89-2495-409f-90d9-4f87afadf148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['notion_pages____dlt_loads', 'notion_pages____dlt_pipeline_state', 'notion_pages____dlt_version', 'notion_pages___dltSentinelTable', 'notion_pages___employee_handbook', 'notion_pages___homework', 'qanda____dlt_loads', 'qanda____dlt_pipeline_state', 'qanda____dlt_version', 'qanda___dltSentinelTable', 'qanda___documents']\n",
      "Number of rows in the 'notion_pages__homework' table: 17\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "\n",
    "# Connect to the lancedb database\n",
    "db = lancedb.connect(\"./.lancedb\")\n",
    "\n",
    "# Check table names to confirm the 'homework' table exists\n",
    "print(db.table_names())\n",
    "\n",
    "# Open the 'notion_pages__homework' table\n",
    "db_table = db.open_table(\"notion_pages___homework\")\n",
    "\n",
    "# Convert the table to a pandas DataFrame and print the number of rows\n",
    "df = db_table.to_pandas()\n",
    "print(f\"Number of rows in the 'notion_pages__homework' table: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1306dea-1d23-45d5-888d-15d022a608c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest last_edited_time stored in the table is: 2024-07-05 23:33:00+00:00\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the lancedb database\n",
    "db = lancedb.connect(\"./.lancedb\")\n",
    "\n",
    "# Open the 'notion_pages__homework' table\n",
    "db_table = db.open_table(\"notion_pages___homework\")\n",
    "\n",
    "# Convert the table to a pandas DataFrame\n",
    "df = db_table.to_pandas()\n",
    "\n",
    "# Perform aggregation to get the maximum value of the 'last_edited_time' column\n",
    "max_last_edited_time = df['last_edited_time'].max()\n",
    "\n",
    "print(f\"The latest last_edited_time stored in the table is: {max_last_edited_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc9e6d3f-9ccf-4b2e-9e07-66dbd1e9116c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Downloading ollama...\n",
      "######################################################################## 100.0%##O#- #                                                                       \n",
      ">>> Installing ollama to /usr/local/bin...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4aee8a5-eb3b-4057-88c4-b6fc3438c7ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Background processes not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnohup ollama serve > nohup.out 2>&1 &\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py:641\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cmd\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# this is *far* from a rigorous test\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# We do not support backgrounding processes because we either use\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# pexpect or pipes to read from.  Users can always just call\u001b[39;00m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;66;03m# os.system() or use ip.system=ip.system_raw\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;66;03m# if they really want a background process.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground processes not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# Also, protect system call from UNC paths on Windows here too\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# as is done in InteractiveShell.system_raw\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOSError\u001b[0m: Background processes not supported."
     ]
    }
   ],
   "source": [
    "!nohup ollama serve > nohup.out 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41c15f0d-69d7-4b2b-bafd-1657eb86e29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!ollama pull llama2-uncensored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54c8f17d-9861-42f0-84e6-aaef5606ffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /home/codespace/.local/lib/python3.10/site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.4.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /home/codespace/.local/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (4.12.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dcc3feee-1731-4270-9846-19b29596a397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  how many PTO days are the employees entitled to in a year?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Based on the context provided, it appears that employees are entitled to 30 days of PTO per year. They can earn an additional day after their first year with the company and have a cap at 25 days overall. If they want to use PTO, they should send a request through HRIS, and if approved, they can take their leave. It is also stated that employees cannot transfer any remaining PTO to the next year.\n",
      "If you need more details about how PTO is handled or want to confirm this information before answering further questions, please contact your company's human resources department.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 44\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m# Add the response into the context window\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         messages\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     41\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_content}\n\u001b[1;32m     42\u001b[0m         )\n\u001b[0;32m---> 44\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[61], line 21\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     16\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant that helps users understand policies inside a company\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms employee handbook. The user will first ask you a question and then provide you relevant paragraphs from the handbook as context. Please answer the question based on the provided context. For any details missing in the paragraph, encourage the employee to contact the HR for that information. Please keep the responses conversational.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     17\u001b[0m ]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Accept user question\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Retrieve the relevant paragraphs on the question\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     context \u001b[38;5;241m=\u001b[39m retrieve_context_from_lancedb(dbtable, question, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "import ollama\n",
    "\n",
    "def retrieve_context_from_lancedb(dbtable, question, top_k=2):\n",
    "    query_results = dbtable.search(query=question).to_list()\n",
    "    context = \"\\n\".join([result[\"content\"] for result in query_results[:top_k]])\n",
    "    return context\n",
    "\n",
    "def main():\n",
    "    # Connect to the lancedb table\n",
    "    db = lancedb.connect(\".lancedb\")\n",
    "    dbtable = db.open_table(\"notion_pages___homework\")\n",
    "\n",
    "    # A system prompt telling ollama to accept input in the form of \"Question: ... ; Context: ...\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that helps users understand policies inside a company's employee handbook. The user will first ask you a question and then provide you relevant paragraphs from the handbook as context. Please answer the question based on the provided context. For any details missing in the paragraph, encourage the employee to contact the HR for that information. Please keep the responses conversational.\"}\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        # Accept user question\n",
    "        question = input(\"You: \")\n",
    "\n",
    "        # Retrieve the relevant paragraphs on the question\n",
    "        context = retrieve_context_from_lancedb(dbtable, question, top_k=2)\n",
    "\n",
    "        # Create a user prompt using the question and retrieved context\n",
    "        messages.append(\n",
    "            {\"role\": \"user\", \"content\": f\"Question: '{question}'; Context:'{context}'\"}\n",
    "        )\n",
    "\n",
    "        # Get the response from the LLM\n",
    "        response = ollama.chat(\n",
    "            model=\"llama2-uncensored\",\n",
    "            messages=messages\n",
    "        )\n",
    "        response_content = response['message']['content']\n",
    "        print(f\"Assistant: {response_content}\")\n",
    "\n",
    "        # Add the response into the context window\n",
    "        messages.append(\n",
    "            {\"role\": \"assistant\", \"content\": response_content}\n",
    "        )\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e49df-2572-4009-90b2-fc98565eaf7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
